---
title: "STAT 301-2 Final Report"
subtitle: |
  | What Indicators Can Predict Social Media Shares of Mashable News Articles?
author: "Stacy Caeiro"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---
```{r}
#| echo: false 
library(tidyverse)
library(tidymodels)
library(here)
library(knitr)
load(here("results/rmse_tibble.rda"))
load(here("results/rf_final_metrics.rda"))
```

## Introduction 
-My prediction problem is to predict the amount of social media shares a news article will receive. 
-I am choosing this problem because increasing social media shares is a common business goal. The results of this model can act as a guide to other publishing sites on how to increase their social media visibility.
-I will be using the [Online News Popularity](https://archive.ics.uci.edu/dataset/332/online+news+popularity) from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/). This
dataset contains features of Mashable news articles, such as length of title and date of the week published, that were published over a 2 year period. 

## Data Overview 
![shares-hist](img/shares_hist.png)
-The target variable is right-skewed, meaning that most news articles receive 1-10,000 shares, and very few articles receive more than 20,000 shares. 
-Furthermore, there are no missing values with any of the variables in the dataset. 
-[insert summary of EDA]

## Methods 
-80-20 data split with stratified sampling (reason: large enough dataset that 20% for testing provides sufficient # of observations while also avoiding overfitting)
-Regression analysis 
-baseline null, linear, elastic net, random forest, k-nearest neighbors, and boosted tree
-knn: tuned neighbors, en: tuned penalty and mixture, bt: tuned min n, mtry (1,50), and learn rate, rf: tuned mtry (1,36), min n
-2 baseline recipes, 2 feature engineered recipes 
-v-fold cross validation with 5 folds and 3 repeats (reason: computer limitations)
-regular grid with 5 levels 
-RMSE as final metric 

## Model Building and Selection
-RMSE will be final metric
```{r}
#| echo: false 

rmse_tibble |>
  knitr::kable()
```

-due to the relatively low RMSE values, further tuning does not need to be explored
for these models 
-In the future, I would set `mtry` to square root of predictors instead of 50-70%
of predictors to limit computational time. 
-For the basic recipe models, the best parameters were: 
 - random forest: `mtry` = 9, `min_n` = 11
 - boosted tree: `mtry` = 37, `min_n` = 30, `learn_rate` = 0.316
 - elastic net: `penalty` = 1.0, `mixture` = 0.05
 - k-nearest neighbors: `neighbors` = 15
-For the feature engineering recipe models, the best parameters were: 
 - random forest: `mtry` = 9, `min_n` = 2
 - boosted tree: `mtry` = 25, `min_n` = 40, `learn_rate` = 0.316
 - elastic net: `penalty` = 0.00316, `mixture` = 0.7625
 - k-nearest neighbors: `neighbors` = 15
-The best performing model was the random forest model with the basic recipe with 
hyperparameters `mtry` = 9 and `min_n` = 2. This is not surprising because in 
our past labs, random forest tends to be the best performing model. However, I 
was surprised that the basic recipe performed better than the feature engineering 
recipe. 

## Final Model Analysis 
```{r}
#| echo: false 
rf_final_metrics |>
  kable()
```
```{r}
#| echo: false 

within_10_pct |>
  kable()
  
```


![pred-vs-shares-log](img/pred_vs_shares_log.png)
```{r}
#| echo: false 

kable(rf_final_metrics_scaled)
```
```{r}
#| echo: false
kable(within_10_pct_scaled)
```

![pred_vs_shares_scaled](img/pred_vs_shares_scaled.png)

[how much better is the final model over the baseline?]
[are there features of the model that make it the best?]

## Conclusion 

## References 

## Appendix: EDA 
