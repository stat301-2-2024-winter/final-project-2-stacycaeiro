---
title: "Predicting Social Media Shares of Mashable News Articles"
subtitle: |
  | STAT 301-2 Final Project
author: "Stacy Caeiro"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---
```{r}
#| echo: false 
library(tidyverse)
library(tidymodels)
library(here)
library(knitr)
```
```{r}
load(here("data_splits/articles_train.rda"))
load(here("results/rmse_tibble.rda"))
load(here("results/rf_final_metrics.rda"))
load(here("results/rf_final_metrics_scaled.rda"))
```

::: {.callout-tip icon="false"}
## Github Repo Link

[Final-Project-Github-Link](https://github.com/stat301-2-2024-winter/final-project-2-stacycaeiro.git)
:::
## Introduction 
My prediction problem is to predict the amount of social media shares that an online article with receive. I am choosing this problem because increasing social media shares is a common business goal. Thus, the results of this model can act as a guide to other publishing sities on how to increase their social media visibility. 

I will be using the [Online News Popularity](https://archive.ics.uci.edu/dataset/332/online+news+popularity) from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/). This dataset contains features of Mashable news articles published over a 2 year period, such as length of title and day of the week published. 

## Data Overview 

### Target Variable Analysis
![Frequency of Social Media Shares](img/shares_hist.png){#fig-shares-hist}
According to @fig-shares-hist, the frequency of shares is right-skewed. Most articles receive less than 10,000 shares and very few articles receive more than 20,000 shares. 

### Exploratory Data Analysis
For starters, the entire dataset contains no missing values. 

Furthermore, upon analysis of a subset of the dataset, which contained 80% of the training data, the following trends were discovered: 
1. There are correlations between predictor variables in the dataset, such as between number of links and number of images in an article
2. Multiple variables, such as number of tokens of content and shares of best keyword, are right skewed 
3. Multiple variables, such as title and global sentiment polarity, have a non-linear relationship. 

I added specific pre-processing steps to my feature engineering recipes to address these trends and make a more efficient model. The graphs supporting these trends are showcased in Appendix I. 

## Methods 

### Data Splitting & Folding
For this predictive model, I will be conducting a regression analysis because the target outcome is a numeric variable. 

I have chosen to conduct v-fold cross-validation in order to mitigate the risk of overfitting the model. I used an 80-20 data split because my dataset has 39k observations, so using 20% of the dataset for testing will provide enough observations for sufficient testing but not so many observations that the model is overfit. Additionally, I used stratified sampling on the target variable shares in order to ensure a balanced distribution of the target variables within all of the folds. Furthermore, I used 5 folds and 3 repeats. I chose these values because 5 fold and 3 repeats allows for a sufficient amount of models to be trained, but low enough the maintain a reasonable computational speed and efficiency. 

### Models & Tuning
I tuned and fit 6 types of models: null, linear, elastic net, random forest, k-nearest neighbors, and boosted tree. 

Here are the parameters I have tuned per model: 
1. Null - no tuning 
2. Linear - no tuning 
3. Elastic Net - tuned penalty and mixture
4. Random Forest - tuned number of variables sampled at each split and minimum number of observations per node 
5. K-Nearest Neighbors - tuned number of neighbors 
6. Boosted Tree - tuned number of variables sampled at each split, minimum number of observations per node, and learn rate 

I manually updated the number of variables sampled at each split to range from 1 to 36 because 36 is the 50% of the amount of predictor variables in my dataset. I chose 50% as the cutoff to maintain reasonable computational speed and efficiency while still allowing for a variety of predictors to be sampled. For all other hyperparameters, I kept the default ranges. 

Furthermore, I tuned my models using a regular grid with 5 levels to explore a variety of ranges of hyperparameter pairings while still maintaining reasonable computational speed and efficiency. 

### Recipes 
I ran each model twice, one on a kitchen sink recipe and one on a feature engineering recipe. I had 4 total recipes: a kitchen sink recipe for non-tree models, a kitchen sink recipe for tree models, a feature engineering recipe for non-tree models, and a feature engineering recipe for tree models. 

I added the following pre-processing steps to my feature engineering recipes: 
1. A Yeo-Johnson transformation to 6 right-skewed numeric variables to address the skew. I chose a Yeo-Johnson transformation over a log transformation because these variables contained negative numbers. 
2. A natural spline transformation to 2 variables to capture the non-linear relationships within them 
3. A correlation transformation to all predictors to remove highly correlated predictor variables in order to reduce redundancy in the model
4. Two interactions to capture relationships between predictor variables that when compounded could potentially improve model efficiency. 

### Metric 
I am using Root Mean Square Error (RMSE) as my final metric because it provides an easily understandable measure of the average prediction error of the model. 

## Model Building and Selection
```{r}
#| label: tbl-RMSE
#| tbl-cap: "RMSE of all Models"
#| echo: false 

rmse_tibble |>
  knitr::kable()
```
Again, I will be using RMSE to compare models and determine the winning model. As seen in @tbl-RMSE , due to the relatively low RMSE values, further tuning does not need to be explored for these models. However, in the future, I would set the number of variables set at each split (`mtry`) to the square root of predictors instead of 50% of predictors in order to limit computational time, as my tree models took 4-7 hours to run. 

For the basic recipe models, the best parameters were: 
 - random forest: `mtry` = 9, `min_n` = 11
 - boosted tree: `mtry` = 37, `min_n` = 30, `learn_rate` = 0.316
 - elastic net: `penalty` = 1.0, `mixture` = 0.05
 - k-nearest neighbors: `neighbors` = 15
 
-For the feature engineering recipe models, the best parameters were: 
 - random forest: `mtry` = 9, `min_n` = 2
 - boosted tree: `mtry` = 25, `min_n` = 40, `learn_rate` = 0.316
 - elastic net: `penalty` = 0.00316, `mixture` = 0.7625
 - k-nearest neighbors: `neighbors` = 15

There was minimal difference between the RMSE of each model with the basic recipe 
compared to the same model with the feature engineering recipe. In fact, the random 
forest, boosted tree, elastic net, and null model performed slightly better with 
the basic recipe than the feature engineering recipe. Furthermore, there were minimal
differences between performance amongst model type, with the RMSE of all models besides the
linear model being within the range of 0.847-0.928. 

Ultimately, the best performing model was the random forest model with the basic recipe and
hyperparameters `mtry` = 9 and `min_n` = 2. This is not surprising because in 
our past labs, random forest tends to be the best performing model. However, I 
was surprised that the basic recipe performed better than the feature engineering 
recipe because I added preprocessing steps to the feature engineering based on my EDA
to attempt to make the model more efficient. 

## Final Model Analysis 
```{r}
#| label: tbl-rf-metrics
#| tbl-cap: "Metrics for Final Random Forest Model"
#| echo: false 
rf_final_metrics |>
  kable()
```
According to @tbl-rf-metrics, the final random forest model has an RMSE of 0.86, an R-squared (RSQ) value of 0.165, and a mean absolute error (MAE) value of 0.631. 
The RMSE of 0.86 indicates that on average, the predicted values deviate from the actual values by 0.86. The somewhat low value of this RMSE means this model has low-moderate accuracy. 

The RSQ of 0.165 indicates that 16.5% of the variance in shares is explained by the model. This low value means that the model poorly captures the relationship between the predictors and the outcome. 

The MAE of 0.631 indicates that the absolute difference between the predicted and actual values is 0.631. The somewhat low value of the metric means this model has low-moderate accuracy. 

```{r}
#| label: tbl-ten-pct
#| tbl-cap: "Percent of Predictions within 10% of Actual Value"
#| echo: false 

within_10_pct |>
  kable()
  
```
According to @tbl-ten-pct, 

![Predicted vs Actual Values on Log Scale](img/pred_vs_shares_log.png){#fig-pred-log}

According to @fig-pred-log, 

```{r}
#| label: tbl-rf-metrics-scaled
#| tbl-cap: "Metrics for Final Model on Original Scale"
#| echo: false 

kable(rf_final_metrics_scaled)
```

According to @tbl-rf-metrics-scaled, 

```{r}
#| label: tbl-ten-pct-scaled
#| tbl-cap: "Percent of Predicted Values Within 10% of Actual Values on Original Scale"
#| echo: false
kable(within_10_pct_scaled)
```

According to @tbl-ten-pct-scaled, 

![Predicted vs Actual Values on Original Scale](img/pred_vs_shares_scaled.png){#fig-pred-scaled}

According to @fig-pred-scaled, 
[how much better is the final model over the baseline?]
[are there features of the model that make it the best?]

## Conclusion 

## References 

## Appendix: EDA 
